{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d1edb3-65c9-4f2b-aa82-5b5fa2ea84a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import lit, col, when, count, mean, format_string, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f479d0f-beb4-45b4-8fe6-730a88f9e6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'flight-status-prediction'\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fdf725b-086b-4a0d-8dc2-9585d1730da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Collection\n",
    "The data was collected from the Kaggle ([link](https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022/)) having total 55 raw files and one Airline file. There are total approx 29 million rows and 119 features. The size of raw data is approx 15GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fdbd807-62f0-487b-aee9-f154a8c07245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 06:21:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gs://flight-status-prediction/data/Flights_2022_8.csv due to error: [PATH_NOT_FOUND] Path does not exist: gs://flight-status-prediction/data/Flights_2022_8.csv.\nSkipping file gs://flight-status-prediction/data/Flights_2022_9.csv due to error: [PATH_NOT_FOUND] Path does not exist: gs://flight-status-prediction/data/Flights_2022_9.csv.\nSkipping file gs://flight-status-prediction/data/Flights_2022_10.csv due to error: [PATH_NOT_FOUND] Path does not exist: gs://flight-status-prediction/data/Flights_2022_10.csv.\nSkipping file gs://flight-status-prediction/data/Flights_2022_11.csv due to error: [PATH_NOT_FOUND] Path does not exist: gs://flight-status-prediction/data/Flights_2022_11.csv.\nSkipping file gs://flight-status-prediction/data/Flights_2022_12.csv due to error: [PATH_NOT_FOUND] Path does not exist: gs://flight-status-prediction/data/Flights_2022_12.csv.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Data\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define the range of years for the dataset\n",
    "years = [2018 + i for i in range(5)]\n",
    "constants = [i + 1 for i in range(12)]\n",
    "\n",
    "combined_df = None\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the years and constants to read files\n",
    "for year in years:\n",
    "    for constant in constants:\n",
    "        file_path = f\"gs://{BUCKET_NAME}/data/Flights_{year}_{constant}.csv\"\n",
    "        \n",
    "        # Check if the file exists before attempting to read it\n",
    "        try:\n",
    "            df = (\n",
    "                spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "                .withColumn(\"year\", lit(year))\n",
    "            )\n",
    "            \n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file_path} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd21976-4785-424d-a5d9-a8d3335695ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:====================================================>(217 + 3) / 220]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 29193782\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into a single DataFrame using unionByName if there are valid DataFrames\n",
    "if dataframes:\n",
    "    combined_df = reduce(DataFrame.unionByName, dataframes)\n",
    "    print(f\"Total records: {combined_df.count()}\")\n",
    "else:\n",
    "    print(\"No valid files were found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f627804c-1939-4bc3-b9ef-46fe19934aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge the two tables and create a one single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62404b44-c9c1-4c07-a85e-5e127271084d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(Code='02Q', Description='Titan Airways')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_df = spark.read.csv(f\"gs://{BUCKET_NAME}/data/Airlines.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc2dabaf-75ad-443e-88eb-e702f5b44778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Columns: ['year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'FlightDate', 'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners', 'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline', 'Flight_Number_Marketing_Airline', 'Originally_Scheduled_Code_Share_Airline', 'DOT_ID_Originally_Scheduled_Code_Share_Airline', 'IATA_Code_Originally_Scheduled_Code_Share_Airline', 'Flight_Num_Originally_Scheduled_Code_Share_Airline', 'Operating_Airline', 'DOT_ID_Operating_Airline', 'IATA_Code_Operating_Airline', 'Tail_Number', 'Flight_Number_Operating_Airline', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac', 'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'Dest', 'DestCityName', 'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'CRSDepTime', 'DepTime', 'DepDelay', 'DepDelayMinutes', 'DepDel15', 'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrDel15', 'ArrivalDelayGroups', 'ArrTimeBlk', 'Cancelled', 'CancellationCode', 'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'Flights', 'Distance', 'DistanceGroup', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime', 'DivArrDelay', 'DivDistance', 'Div1Airport', 'Div1AirportID', 'Div1AirportSeqID', 'Div1WheelsOn', 'Div1TotalGTime', 'Div1LongestGTime', 'Div1WheelsOff', 'Div1TailNum', 'Div2Airport', 'Div2AirportID', 'Div2AirportSeqID', 'Div2WheelsOn', 'Div2TotalGTime', 'Div2LongestGTime', 'Div2WheelsOff', 'Div2TailNum', 'Div3Airport', 'Div3AirportID', 'Div3AirportSeqID', 'Div3WheelsOn', 'Div3TotalGTime', 'Div3LongestGTime', 'Div3WheelsOff', 'Div3TailNum', 'Div4Airport', 'Div4AirportID', 'Div4AirportSeqID', 'Div4WheelsOn', 'Div4TotalGTime', 'Div4LongestGTime', 'Div4WheelsOff', 'Div4TailNum', 'Div5Airport', 'Div5AirportID', 'Div5AirportSeqID', 'Div5WheelsOn', 'Div5TotalGTime', 'Div5LongestGTime', 'Div5WheelsOff', 'Div5TailNum', 'Duplicate', '_c119']\n"
     ]
    }
   ],
   "source": [
    "# Clean column names to remove spaces or special characters\n",
    "combined_df = combined_df.select(\n",
    "    [col(c).alias(c.strip().replace(\" \", \"_\")) for c in combined_df.columns]\n",
    ")\n",
    "\n",
    "# Re-check column names\n",
    "print(\"Cleaned Columns:\", combined_df.columns)\n",
    "\n",
    "# Perform the left join with necessary columns\n",
    "result_df = combined_df.join(\n",
    "    airline_df.select(\"Code\", \"Description\"),\n",
    "    combined_df[\"Operating_Airline\"] == airline_df[\"Code\"],\n",
    "    \"left\"\n",
    ").drop(\"Code\").withColumnRenamed(\"Description\", \"Airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e676c7-c6fe-4482-b355-72a7a802b424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 06:24:59 WARN DAGScheduler: Broadcasting large task binary with size 1786.9 KiB\n24/12/15 06:28:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n[Stage 124:====================================================>(438 + 2) / 440]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining columns: 68\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate null counts and percentages in a single pass\n",
    "null_counts = result_df.selectExpr(\n",
    "    *[f\"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}\" for c in result_df.columns]\n",
    ").first()\n",
    "\n",
    "# Calculate total rows\n",
    "total_rows = result_df.count()\n",
    "\n",
    "# Identify columns to drop based on the threshold\n",
    "columns_to_drop = [\n",
    "    c for c in result_df.columns if null_counts[c] / total_rows > threshold\n",
    "]\n",
    "\n",
    "# Drop columns with high null percentages\n",
    "df_cleaned = result_df.drop(*columns_to_drop)\n",
    "\n",
    "# Output the number of remaining columns\n",
    "print(f\"Number of remaining columns: {len(df_cleaned.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8209107a-cc75-4db8-a2ef-b139f28e0ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[year: int, Month: int, DayofMonth: int, DayOfWeek: int, FlightDate: date, Operated_or_Branded_Code_Share_Partners: string, DOT_ID_Operating_Airline: int, Origin: string, OriginCityName: string, OriginState: string, Dest: string, DestCityName: string, DestState: string, CRSDepTime: int, DepTime: int, DepDelay: double, DepDelayMinutes: double, DepartureDelayGroups: int, TaxiOut: double, TaxiIn: double, CRSArrTime: int, ArrTime: int, ArrDelay: double, ArrDelayMinutes: double, ArrivalDelayGroups: int, Cancelled: double, CRSElapsedTime: double, ActualElapsedTime: double, AirTime: double, Flights: double, Distance: double, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double, DivAirportLandings: int, Duplicate: string, Airline: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.drop(*[\"Diverted\", \"Quarter\", \"Marketing_Airline_Network\", \"Marketing_Airline_Network\", \"DOT_ID_Marketing_Airline\", \"IATA_Code_Marketing_Airline\", \"Flight_Number_Marketing_Airline\", \"Operating_Airline\", \"IATA_Code_Operating_Airline\", \"Tail_Number\", \"Flight_Number_Operating_Airline\", \"OriginAirportID\", \"OriginAirportSeqID\", \"OriginCityMarketID\", \"OriginStateFips\", \"OriginStateName\", \"OriginWac\", \"DestAirportID\", \"DestAirportSeqID\", \"DestCityMarketID\", \"DestStateFips\", \"DestStateName\", \"DestWac\", \"DepDel15\", \"DepTimeBlk\", \"WheelsOff\", \"WheelsOn\", \"ArrDel15\", \"ArrTimeBlk\", \"DistanceGroup\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa68ff7-d660-45a8-8aa3-2fce153ba094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 06:29:45 WARN DAGScheduler: Broadcasting large task binary with size 1290.7 KiB\n[Stage 132:====================================================>(438 + 2) / 440]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DepTime' has 2.61% null values.\nColumn 'DepDelay' has 2.61% null values.\nColumn 'DepDelayMinutes' has 2.61% null values.\nColumn 'DepartureDelayGroups' has 2.61% null values.\nColumn 'TaxiOut' has 2.67% null values.\nColumn 'TaxiIn' has 2.72% null values.\nColumn 'ArrTime' has 2.69% null values.\nColumn 'ArrDelay' has 2.90% null values.\nColumn 'ArrDelayMinutes' has 2.90% null values.\nColumn 'ArrivalDelayGroups' has 2.90% null values.\nColumn 'CRSElapsedTime' has 0.00% null values.\nColumn 'ActualElapsedTime' has 2.90% null values.\nColumn 'AirTime' has 2.92% null values.\nColumn 'CarrierDelay' has 82.85% null values.\nColumn 'WeatherDelay' has 82.85% null values.\nColumn 'NASDelay' has 82.85% null values.\nColumn 'SecurityDelay' has 82.85% null values.\nColumn 'LateAircraftDelay' has 82.85% null values.\nColumn 'DivAirportLandings' has 0.00% null values.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count once\n",
    "total_count = df_cleaned.count()\n",
    "\n",
    "# Compute null counts for all columns in a single pass\n",
    "null_counts = df_cleaned.select([\n",
    "    (count(when(col(c).isNull(), c)) / total_count * 100).alias(c) for c in df_cleaned.columns\n",
    "])\n",
    "\n",
    "# Collect the results to the driver\n",
    "null_columns = [\n",
    "    (col_name, percentage) \n",
    "    for col_name, percentage in null_counts.first().asDict().items() \n",
    "    if percentage > 0\n",
    "]\n",
    "\n",
    "# Print columns with null values\n",
    "if null_columns:\n",
    "    for column, percentage in null_columns:\n",
    "        print(f\"Column '{column}' has {percentage:.2f}% null values.\")\n",
    "else:\n",
    "    print(\"No columns have null values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c6a3a84-c503-4316-9e89-bf996d884e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Operated_or_Branded_Code_Share_Partners',\n",
       " 'DOT_ID_Operating_Airline',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'CRSDepTime',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'DepartureDelayGroups',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'CRSArrTime',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'ArrivalDelayGroups',\n",
       " 'Cancelled',\n",
       " 'CRSElapsedTime',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'DivAirportLandings',\n",
       " 'Duplicate',\n",
       " 'Airline']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.drop(*[\"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95caa911-6ebc-466b-b9f9-f45b3fa63b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop rows that have null values\n",
    "df_cleaned = df_cleaned.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3a75c52-f48d-4590-bf71-162e4499b318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[year: int, Month: int, DayofMonth: int, DayOfWeek: int, FlightDate: date, Operated_or_Branded_Code_Share_Partners: string, DOT_ID_Operating_Airline: int, Origin: string, OriginCityName: string, OriginState: string, Dest: string, DestCityName: string, DestState: string, CRSDepTime: string, DepTime: string, DepDelay: double, DepDelayMinutes: double, DepartureDelayGroups: int, TaxiOut: double, TaxiIn: double, CRSArrTime: string, ArrTime: string, ArrDelay: double, ArrDelayMinutes: double, ArrivalDelayGroups: int, Cancelled: double, CRSElapsedTime: double, ActualElapsedTime: double, AirTime: double, Flights: double, Distance: double, DivAirportLandings: int, Duplicate: string, Airline: string, time_of_day: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"DepTime\",\n",
    "                   format_string(\"%02d:%02d\",\n",
    "                                   (col(\"DepTime\") / 100).cast(\"int\"),  # Extract hours\n",
    "                                   (col(\"DepTime\") % 100).cast(\"int\")   # Extract minutes\n",
    "                                  )\n",
    "                  )\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"CRSDepTime\",\n",
    "                   format_string(\"%02d:%02d\",\n",
    "                                   (col(\"CRSDepTime\") / 100).cast(\"int\"),  # Extract hours\n",
    "                                   (col(\"CRSDepTime\") % 100).cast(\"int\")   # Extract minutes\n",
    "                                  )\n",
    "                  )\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"ArrTime\",\n",
    "                   format_string(\"%02d:%02d\",\n",
    "                                   (col(\"ArrTime\") / 100).cast(\"int\"),  # Extract hours\n",
    "                                   (col(\"ArrTime\") % 100).cast(\"int\")   # Extract minutes\n",
    "                                  )\n",
    "                  )\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"CRSArrTime\",\n",
    "                   format_string(\"%02d:%02d\",\n",
    "                                   (col(\"CRSArrTime\") / 100).cast(\"int\"),  # Extract hours\n",
    "                                   (col(\"CRSArrTime\") % 100).cast(\"int\")   # Extract minutes\n",
    "                                  )\n",
    "                  )\n",
    "\n",
    "# Show the result\n",
    "df_cleaned = df_cleaned.withColumn(\"flight_hour\", substring(col(\"DepTime\"), 1, 2).cast(\"int\"))\n",
    "\n",
    "# Classify the time into Early Morning, Morning, Afternoon, or Night\n",
    "df_cleaned = df_cleaned.withColumn(\"time_of_day\",\n",
    "                   when((col(\"flight_hour\") >= 0) & (col(\"flight_hour\") < 6), \"Early Morning\")\n",
    "                    .when((col(\"flight_hour\") >= 6) & (col(\"flight_hour\") < 12), \"Morning\")\n",
    "                    .when((col(\"flight_hour\") >= 12) & (col(\"flight_hour\") < 16), \"Afternoon\")\n",
    "                    .when((col(\"flight_hour\") >= 16) & (col(\"flight_hour\") < 20), \"Evening\")\n",
    "                    .when((col(\"flight_hour\") >= 20) & (col(\"flight_hour\") < 24), \"Night\")\n",
    "                    .otherwise(\"Unknown\"))\n",
    "\n",
    "# Drop the temporary flight_hour column if not needed\n",
    "df_cleaned = df_cleaned.drop(\"flight_hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfdcd0d-a271-4943-acdd-a7d6c806013b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:====================================================>(438 + 2) / 440]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns have null values.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count once\n",
    "total_count = df_cleaned.count()\n",
    "\n",
    "# Compute null counts for all columns in a single pass\n",
    "null_counts = df_cleaned.select([\n",
    "    (count(when(col(c).isNull(), c)) / total_count * 100).alias(c) for c in df_cleaned.columns\n",
    "])\n",
    "\n",
    "# Collect the results to the driver\n",
    "null_columns = [\n",
    "    (col_name, percentage) \n",
    "    for col_name, percentage in null_counts.first().asDict().items() \n",
    "    if percentage > 0\n",
    "]\n",
    "\n",
    "# Print columns with null values\n",
    "if null_columns:\n",
    "    for column, percentage in null_columns:\n",
    "        print(f\"Column '{column}' has {percentage:.2f}% null values.\")\n",
    "else:\n",
    "    print(\"No columns have null values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b29d93-68bc-476d-84d5-be1f09148399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 06:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1393.8 KiB\n                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = f\"gs://{BUCKET_NAME}/cleaned_data\"\n",
    "df_cleaned.coalesce(1).write.mode('overwrite').parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec51a2aa-269d-4410-9d7f-318e31db5052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final data cleaning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
